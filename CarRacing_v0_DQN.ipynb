{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CarRacing_v0_DQN_.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkdhogpi-dzS",
        "colab_type": "text"
      },
      "source": [
        "#導入Box.2D相關套件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meNgbYl49tKB",
        "colab_type": "code",
        "outputId": "90fb4842-d535-40a0-f40f-1e72190401a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple box2d-py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 249kB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VzyydQJ9wWF",
        "colab_type": "code",
        "outputId": "ac6861ba-6273-4651-b877-697a1597115f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "!apt-get install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install Pillow\n",
        "!pip install 'gym[box2d]'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 783 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.3 [783 kB]\n",
            "Fetched 783 kB in 1s (725 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 145674 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.3_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/69/ec/8221a07850d69fa3c57c02e526edd23d18c7c05d58ed103e3b19172757c1/PyVirtualDisplay-0.2.5-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/3d/47/9559808446353ef6436c13e7b1475f90ce068e89c600add1c8f219063fcc/EasyProcess-0.2.9-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.2.9 pyvirtualdisplay-0.2.5\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (6.2.2)\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (4.1.2.30)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym[box2d]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70vmnYHG9wYk",
        "colab_type": "code",
        "outputId": "2ff45298-b03d-4496-9501-17ab2df0fee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anyTor2X-pAd",
        "colab_type": "text"
      },
      "source": [
        "#導入相關函式庫"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjHxVq7w9wbD",
        "colab_type": "code",
        "outputId": "9bbbff25-eb6a-4a69-af36-9662a948cd9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow.compat.v2 as tf\n",
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjO5iyeZ-vlX",
        "colab_type": "text"
      },
      "source": [
        "#Import Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW4yFn2u9wdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "from gym.wrappers import Monitor\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW26bKbB-2cq",
        "colab_type": "text"
      },
      "source": [
        "#Main_採用隨機動作"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPAqj64A9whg",
        "colab_type": "code",
        "outputId": "22a0a2f2-ff2b-47e7-9520-ee5d11d7bf64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "import gym\n",
        "import time\n",
        "from IPython import display\n",
        "from PIL import Image\n",
        "score = 0\n",
        "t = 0\n",
        "\n",
        "env = wrap_env(gym.make('CarRacing-v0'))\n",
        "env.reset()\n",
        "\n",
        "for episode in range(5):\n",
        "  observation = env.reset()  \n",
        "  for i in range(1000): \n",
        "    action = env.action_space.sample()   # 從 action_space 隨機採樣一個動作\n",
        "    observation, reward, done, info = env.step(action)   # 執行動作\n",
        "    #print(f'Action {action} Reward {reward}')\n",
        "    score += reward\n",
        "            \n",
        "    \n",
        "    \n",
        "    if done:\n",
        "        print('episode',t+1,'scores = ',score)\n",
        "        score = 0\n",
        "        t += 1\n",
        "        break\n",
        "        \n",
        "env.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Track generation: 1129..1423 -> 294-tiles track\n",
            "Track generation: 1034..1304 -> 270-tiles track\n",
            "episode 1 scores =  -29.36802973977737\n",
            "Track generation: 1147..1438 -> 291-tiles track\n",
            "episode 2 scores =  -31.03448275862115\n",
            "Track generation: 1186..1486 -> 300-tiles track\n",
            "episode 3 scores =  -33.110367892977024\n",
            "Track generation: 1035..1298 -> 263-tiles track\n",
            "episode 4 scores =  -27.480916030534658\n",
            "Track generation: 1088..1364 -> 276-tiles track\n",
            "episode 5 scores =  -27.27272727272763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flAMbTNZ-6bB",
        "colab_type": "text"
      },
      "source": [
        "#ExperienceHistory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dgyuz3z98Tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class ExperienceHistory:\n",
        "    \"\"\"\n",
        "    This saves the agent's experience in windowed cache.\n",
        "    Each frame is saved only once but state is stack of num_frame_stack frames\n",
        "    In the beginning of an episode the frame-stack is padded\n",
        "    with the beginning frame\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "            num_frame_stack=4,\n",
        "            capacity=int(1e5),\n",
        "            pic_size=(96, 96)\n",
        "    ):\n",
        "        self.num_frame_stack = num_frame_stack\n",
        "        self.capacity = capacity\n",
        "        self.pic_size = pic_size\n",
        "        self.counter = 0\n",
        "        self.frame_window = None\n",
        "        self.init_caches()\n",
        "        self.expecting_new_episode = True\n",
        "\n",
        "    def add_experience(self, frame, action, done, reward):\n",
        "        assert self.frame_window is not None, \"start episode first\"\n",
        "        self.counter += 1\n",
        "        frame_idx = self.counter % self.max_frame_cache\n",
        "        exp_idx = (self.counter - 1) % self.capacity\n",
        "\n",
        "        self.prev_states[exp_idx] = self.frame_window\n",
        "        self.frame_window = np.append(self.frame_window[1:], frame_idx)\n",
        "        self.next_states[exp_idx] = self.frame_window\n",
        "        self.actions[exp_idx] = action\n",
        "        self.is_done[exp_idx] = done\n",
        "        self.frames[frame_idx] = frame\n",
        "        self.rewards[exp_idx] = reward\n",
        "        if done:\n",
        "            self.expecting_new_episode = True\n",
        "\n",
        "    def start_new_episode(self, frame):\n",
        "        # it should be okay not to increment counter here\n",
        "        # because episode ending frames are not used\n",
        "        assert self.expecting_new_episode, \"previous episode didn't end yet\"\n",
        "        frame_idx = self.counter % self.max_frame_cache\n",
        "        self.frame_window = np.repeat(frame_idx, self.num_frame_stack)\n",
        "        self.frames[frame_idx] = frame\n",
        "        self.expecting_new_episode = False\n",
        "\n",
        "    def sample_mini_batch(self, n):\n",
        "        count = min(self.capacity, self.counter)\n",
        "        batchidx = np.random.randint(count, size=n)\n",
        "\n",
        "        prev_frames = self.frames[self.prev_states[batchidx]]\n",
        "        next_frames = self.frames[self.next_states[batchidx]]\n",
        "\n",
        "        return {\n",
        "            \"reward\": self.rewards[batchidx],\n",
        "            \"prev_state\": prev_frames,\n",
        "            \"next_state\": next_frames,\n",
        "            \"actions\": self.actions[batchidx],\n",
        "            \"done_mask\": self.is_done[batchidx]\n",
        "        }\n",
        "\n",
        "    def current_state(self):\n",
        "        # assert not self.expecting_new_episode, \"start new episode first\"'\n",
        "        assert self.frame_window is not None, \"do something first\"\n",
        "        return self.frames[self.frame_window]\n",
        "\n",
        "    def init_caches(self):\n",
        "        self.rewards = np.zeros(self.capacity, dtype=\"float32\")\n",
        "        self.prev_states = -np.ones((self.capacity, self.num_frame_stack),\n",
        "            dtype=\"int32\")\n",
        "        self.next_states = -np.ones((self.capacity, self.num_frame_stack),\n",
        "            dtype=\"int32\")\n",
        "        self.is_done = -np.ones(self.capacity, \"int32\")\n",
        "        self.actions = -np.ones(self.capacity, dtype=\"int32\")\n",
        "\n",
        "        # lazy to think how big is the smallest possible number. At least this is big enough\n",
        "        self.max_frame_cache = self.capacity + 2 * self.num_frame_stack + 1\n",
        "        self.frames = -np.ones((self.max_frame_cache,) + self.pic_size, dtype=\"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i13zsmmz-9CY",
        "colab_type": "text"
      },
      "source": [
        "#Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cfnBSE198WN",
        "colab_type": "code",
        "outputId": "450af59b-1e92-4563-b25f-25701739f4b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "import numpy as np\n",
        "from skimage import color, transform\n",
        "import tensorflow.contrib.slim as slim\n",
        "import tensorflow as tf\n",
        "import itertools as it\n",
        "#from dqn.experience_history import ExperienceHistory\n",
        "\n",
        "class DQN:\n",
        "    \"\"\"\n",
        "    General DQN agent.\n",
        "    Can be applied to any standard environment\n",
        "    The implementation follows:\n",
        "    Mnih et. al - Playing Atari with Deep Reinforcement Learning https://arxiv.org/pdf/1312.5602.pdf\n",
        "    The q-network structure is different from the original paper\n",
        "    see also:\n",
        "    David Silver's RL course lecture 6: https://www.youtube.com/watch?v=UoPei5o4fps&t=1s\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "            env,\n",
        "            batchsize=64,\n",
        "            pic_size=(96, 96),\n",
        "            num_frame_stack=4,\n",
        "            gamma=0.95,\n",
        "            frame_skip=1,\n",
        "            train_freq=4,\n",
        "            initial_epsilon=1.0,\n",
        "            min_epsilon=0.1,\n",
        "            render=True,\n",
        "            epsilon_decay_steps=int(1e6),\n",
        "            min_experience_size=int(1e3),\n",
        "            experience_capacity=int(1e5),\n",
        "            network_update_freq=5000,\n",
        "            regularization = 1e-6,\n",
        "            optimizer_params = None,\n",
        "            action_map=None\n",
        "    ):\n",
        "        self.exp_history = ExperienceHistory(\n",
        "            num_frame_stack,\n",
        "            capacity=experience_capacity,\n",
        "            pic_size=pic_size\n",
        "        )\n",
        "\n",
        "        # in playing mode we don't store the experience to agent history\n",
        "        # but this cache is still needed to get the current frame stack\n",
        "        self.playing_cache = ExperienceHistory(\n",
        "            num_frame_stack,\n",
        "            capacity=num_frame_stack * 5 + 10,\n",
        "            pic_size=pic_size\n",
        "        )\n",
        "\n",
        "        if action_map is not None:\n",
        "            self.dim_actions = len(action_map)\n",
        "        else:\n",
        "            self.dim_actions = env.action_space.n\n",
        "\n",
        "        self.network_update_freq = network_update_freq\n",
        "        self.action_map = action_map\n",
        "        self.env = env\n",
        "        self.batchsize = batchsize\n",
        "        self.num_frame_stack = num_frame_stack\n",
        "        self.gamma = gamma\n",
        "        self.frame_skip = frame_skip\n",
        "        self.train_freq = train_freq\n",
        "        self.initial_epsilon = initial_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.epsilon_decay_steps = epsilon_decay_steps\n",
        "        self.render = render\n",
        "        self.min_experience_size = min_experience_size\n",
        "        self.pic_size = pic_size\n",
        "        self.regularization = regularization\n",
        "        # These default magic values always work with Adam\n",
        "        self.optimizer_params = optimizer_params or dict(learning_rate=0.0004, epsilon=1e-7)\n",
        "\n",
        "        self.do_training = True\n",
        "        self.playing_epsilon = 0.0\n",
        "        self.session = None\n",
        "\n",
        "        self.state_size = (self.num_frame_stack,) + self.pic_size\n",
        "        self.global_counter = 0\n",
        "        self.episode_counter = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def process_image(img):\n",
        "        return 2 * color.rgb2gray(transform.rescale(img[34:194], 0.5)) - 1\n",
        "\n",
        "    def build_graph(self):\n",
        "        input_dim_with_batch = (self.batchsize, self.num_frame_stack) + self.pic_size\n",
        "        input_dim_general = (None, self.num_frame_stack) + self.pic_size\n",
        "\n",
        "        self.input_prev_state = tf.placeholder(tf.float32, input_dim_general, \"prev_state\")\n",
        "        self.input_next_state = tf.placeholder(tf.float32, input_dim_with_batch, \"next_state\")\n",
        "        self.input_reward = tf.placeholder(tf.float32, self.batchsize, \"reward\")\n",
        "        self.input_actions = tf.placeholder(tf.int32, self.batchsize, \"actions\")\n",
        "        self.input_done_mask = tf.placeholder(tf.int32, self.batchsize, \"done_mask\")\n",
        "\n",
        "        # These are the state action values for all states\n",
        "        # The target Q-values come from the fixed network\n",
        "        with tf.variable_scope(\"fixed\"):\n",
        "            qsa_targets = self.create_network(self.input_next_state, trainable=False)\n",
        "\n",
        "        with tf.variable_scope(\"train\"):\n",
        "            qsa_estimates = self.create_network(self.input_prev_state, trainable=True)\n",
        "\n",
        "        self.best_action = tf.argmax(qsa_estimates, axis=1)\n",
        "\n",
        "        not_done = tf.cast(tf.logical_not(tf.cast(self.input_done_mask, \"bool\")), \"float32\")\n",
        "        q_target = tf.reduce_max(qsa_targets, -1) * self.gamma * not_done + self.input_reward\n",
        "        # select the chosen action from each row\n",
        "        # in numpy this is qsa_estimates[range(batchsize), self.input_actions]\n",
        "        action_slice = tf.stack([tf.range(0, self.batchsize), self.input_actions], axis=1)\n",
        "        q_estimates_for_input_action = tf.gather_nd(qsa_estimates, action_slice)\n",
        "\n",
        "        training_loss = tf.nn.l2_loss(q_target - q_estimates_for_input_action) / self.batchsize\n",
        "\n",
        "        optimizer = tf.train.AdamOptimizer(**(self.optimizer_params))\n",
        "\n",
        "        reg_loss = tf.add_n(tf.losses.get_regularization_losses())\n",
        "        self.train_op = optimizer.minimize(reg_loss + training_loss)\n",
        "\n",
        "        train_params = self.get_variables(\"train\")\n",
        "        fixed_params = self.get_variables(\"fixed\")\n",
        "\n",
        "        assert (len(train_params) == len(fixed_params))\n",
        "        self.copy_network_ops = [tf.assign(fixed_v, train_v)\n",
        "            for train_v, fixed_v in zip(train_params, fixed_params)]\n",
        "\n",
        "    def get_variables(self, scope):\n",
        "        vars = [t for t in tf.global_variables()\n",
        "            if \"%s/\" % scope in t.name and \"Adam\" not in t.name]\n",
        "        return sorted(vars, key=lambda v: v.name)\n",
        "\n",
        "    def create_network(self, input, trainable):\n",
        "        if trainable:\n",
        "            wr = slim.l2_regularizer(self.regularization)\n",
        "        else:\n",
        "            wr = None\n",
        "\n",
        "        # the input is stack of black and white frames.\n",
        "        # put the stack in the place of channel (last in tf)\n",
        "        input_t = tf.transpose(input, [0, 2, 3, 1])\n",
        "\n",
        "        net = slim.conv2d(input_t, 8, (7, 7), data_format=\"NHWC\",\n",
        "            activation_fn=tf.nn.relu, stride=3, weights_regularizer=wr, trainable=trainable)\n",
        "        net = slim.max_pool2d(net, 2, 2)\n",
        "        net = slim.conv2d(net, 16, (3, 3), data_format=\"NHWC\",\n",
        "            activation_fn=tf.nn.relu, weights_regularizer=wr, trainable=trainable)\n",
        "        net = slim.max_pool2d(net, 2, 2)\n",
        "        net = slim.flatten(net)\n",
        "        net = slim.fully_connected(net, 256, activation_fn=tf.nn.relu,\n",
        "            weights_regularizer=wr, trainable=trainable)\n",
        "        q_state_action_values = slim.fully_connected(net, self.dim_actions,\n",
        "            activation_fn=None, weights_regularizer=wr, trainable=trainable)\n",
        "\n",
        "        return q_state_action_values\n",
        "\n",
        "    def check_early_stop(self, reward, totalreward):\n",
        "        return False, 0.0\n",
        "\n",
        "    def get_random_action(self):\n",
        "        return np.random.choice(self.dim_actions)\n",
        "\n",
        "    def get_epsilon(self):\n",
        "        if not self.do_training:\n",
        "            return self.playing_epsilon\n",
        "        elif self.global_counter >= self.epsilon_decay_steps:\n",
        "            return self.min_epsilon\n",
        "        else:\n",
        "            # linear decay\n",
        "            r = 1.0 - self.global_counter / float(self.epsilon_decay_steps)\n",
        "            return self.min_epsilon + (self.initial_epsilon - self.min_epsilon) * r\n",
        "\n",
        "    def train(self):\n",
        "        batch = self.exp_history.sample_mini_batch(self.batchsize)\n",
        "\n",
        "        fd = {\n",
        "            self.input_reward: \"reward\",\n",
        "            self.input_prev_state: \"prev_state\",\n",
        "            self.input_next_state: \"next_state\",\n",
        "            self.input_actions: \"actions\",\n",
        "            self.input_done_mask: \"done_mask\"\n",
        "        }\n",
        "        fd1 = {ph: batch[k] for ph, k in fd.items()}\n",
        "        self.session.run([self.train_op], fd1)\n",
        "\n",
        "    def play_episode(self):\n",
        "        eh = (\n",
        "            self.exp_history if self.do_training\n",
        "            else self.playing_cache\n",
        "        )\n",
        "        total_reward = 0\n",
        "        frames_in_episode = 0\n",
        "\n",
        "        first_frame = self.env.reset()\n",
        "        first_frame_pp = self.process_image(first_frame)\n",
        "\n",
        "        eh.start_new_episode(first_frame_pp)\n",
        "\n",
        "        while True:\n",
        "            if np.random.rand() > self.get_epsilon():\n",
        "                action_idx = self.session.run(\n",
        "                    self.best_action,\n",
        "                    {self.input_prev_state: eh.current_state()[np.newaxis, ...]}\n",
        "                )[0]\n",
        "            else:\n",
        "                action_idx = self.get_random_action()\n",
        "\n",
        "            if self.action_map is not None:\n",
        "                action = self.action_map[action_idx]\n",
        "            else:\n",
        "                action = action_idx\n",
        "\n",
        "            reward = 0\n",
        "            for _ in range(self.frame_skip):\n",
        "                observation, r, done, info = self.env.step(action)\n",
        "                if self.render:\n",
        "                    self.env.render()\n",
        "                reward += r\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            early_done, punishment = self.check_early_stop(reward, total_reward)\n",
        "            if early_done:\n",
        "                reward += punishment\n",
        "\n",
        "            done = done or early_done\n",
        "\n",
        "            total_reward += reward\n",
        "            frames_in_episode += 1\n",
        "\n",
        "            eh.add_experience(self.process_image(observation), action_idx, done, reward)\n",
        "\n",
        "            if self.do_training:\n",
        "                self.global_counter += 1\n",
        "                if self.global_counter % self.network_update_freq:\n",
        "                    self.update_target_network()\n",
        "                train_cond = (\n",
        "                    self.exp_history.counter >= self.min_experience_size and\n",
        "                    self.global_counter % self.train_freq == 0\n",
        "                )\n",
        "                if train_cond:\n",
        "                    self.train()\n",
        "\n",
        "            if done:\n",
        "                if self.do_training:\n",
        "                    self.episode_counter += 1\n",
        "\n",
        "                return total_reward, frames_in_episode\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.session.run(self.copy_network_ops)\n",
        "\n",
        "\n",
        "class CarRacingDQN(DQN):\n",
        "    \"\"\"\n",
        "    CarRacing specifig part of the DQN-agent\n",
        "    Some minor env-specifig tweaks but overall\n",
        "    assumes very little knowledge from the environment\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_negative_rewards=100, **kwargs):\n",
        "        all_actions = np.array(\n",
        "            [k for k in it.product([-1, 0, 1], [1, 0], [0.2, 0])]\n",
        "        )\n",
        "        # car racing env gives wrong pictures without render\n",
        "        kwargs[\"render\"] = True\n",
        "        super().__init__(\n",
        "            action_map=all_actions,\n",
        "            pic_size=(96, 96),\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        self.gas_actions = np.array([a[1] == 1 and a[2] == 0 for a in all_actions])\n",
        "        self.break_actions = np.array([a[2] == 1 for a in all_actions])\n",
        "        self.n_gas_actions = self.gas_actions.sum()\n",
        "        self.neg_reward_counter = 0\n",
        "        self.max_neg_rewards = max_negative_rewards\n",
        "\n",
        "    @staticmethod\n",
        "    def process_image(obs):\n",
        "        return 2 * color.rgb2gray(obs) - 1.0\n",
        "\n",
        "    def get_random_action(self):\n",
        "        \"\"\"\n",
        "        Here random actions prefer gas to break\n",
        "        otherwise the car can never go anywhere.\n",
        "        \"\"\"\n",
        "        action_weights = 14.0 * self.gas_actions + 1.0\n",
        "        action_weights /= np.sum(action_weights)\n",
        "\n",
        "        return np.random.choice(self.dim_actions, p=action_weights)\n",
        "\n",
        "    def check_early_stop(self, reward, totalreward):\n",
        "        if reward < 0:\n",
        "            self.neg_reward_counter += 1\n",
        "            done = (self.neg_reward_counter > self.max_neg_rewards)\n",
        "\n",
        "            if done and totalreward <= 500:\n",
        "                punishment = -20.0\n",
        "            else:\n",
        "                punishment = 0.0\n",
        "            if done:\n",
        "                self.neg_reward_counter = 0\n",
        "\n",
        "            return done, punishment\n",
        "        else:\n",
        "            self.neg_reward_counter = 0\n",
        "            return False, 0.0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mQnmPnI_Pic",
        "colab_type": "text"
      },
      "source": [
        "#test_experienceHistory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91VuWCKS98Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get_ipython().magic(u'load_ext autoreload')\n",
        "#get_ipython().magic(u'autoreload 2')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "from unittest import TestCase\n",
        "#from dqn.experience_history import ExperienceHistory\n",
        "import numpy as np\n",
        "\n",
        "num_frame_stack = 3\n",
        "size = 10\n",
        "pic_size = (2, 2)\n",
        "\n",
        "class TestExperienceHistory(TestCase):\n",
        "    def test_add_frame(self):\n",
        "        h = ExperienceHistory(\n",
        "            num_frame_stack=num_frame_stack,\n",
        "            capacity=size,\n",
        "            pic_size=pic_size\n",
        "        )\n",
        "\n",
        "        #can't do anything because no episode started\n",
        "        with self.assertRaises(AssertionError):\n",
        "            h.current_state()\n",
        "        with self.assertRaises(AssertionError):\n",
        "            h.add_experience(None, None, None, None)\n",
        "\n",
        "        frames = np.random.rand(4, 2, 2).astype(\"float32\")\n",
        "\n",
        "        # add the beginning frame\n",
        "        h.start_new_episode(frames[0])\n",
        "\n",
        "        # Check that padding works correctly\n",
        "        assert (h.current_state() == frames[0]).all()\n",
        "        assert (h.current_state().shape == (num_frame_stack,) + pic_size)\n",
        "\n",
        "        # Now add next frame.\n",
        "        # The action is action taken before this frame\n",
        "        # and reward is the reward observed for this action\n",
        "        # done is a flag if we ended in terminal state\n",
        "        h.add_experience(frames[1], 4, False, 1.0)\n",
        "\n",
        "        assert (h.current_state()[:2] == frames[0]).all()\n",
        "        assert (h.current_state()[2] == frames[1]).all()\n",
        "        assert (h.current_state().shape == (num_frame_stack,) + pic_size)\n",
        "\n",
        "        # add one more experience and set episode as finished\n",
        "        h.add_experience(frames[2], 5, True, 2.0)\n",
        "\n",
        "        # now there should not be any padding for current state\n",
        "        assert (h.current_state() == frames[:3]).all()\n",
        "        assert (h.current_state().shape == (num_frame_stack,) + pic_size)\n",
        "\n",
        "        assert np.all(h.next_states[:3] == np.array([[0, 0, 1], [0, 1, 2], [-1, -1, -1]]))\n",
        "        assert np.all(h.prev_states[:3] == np.array([[0, 0, 0], [0, 0, 1], [-1, -1, -1]]))\n",
        "\n",
        "        h.start_new_episode(frames[3])\n",
        "\n",
        "        assert (h.current_state() == frames[3]).all()\n",
        "        assert (h.current_state().shape == (num_frame_stack,) + pic_size)\n",
        "\n",
        "        batch = h.sample_mini_batch(20)\n",
        "\n",
        "        # Check that we don't sample from the part which is not yet written\n",
        "        # i.e shouldn't see zeros (the caches are initialized with zeros)\n",
        "        assert np.all(np.in1d(batch[\"reward\"], [1., 2.]))\n",
        "        assert np.all(np.in1d(batch[\"actions\"], [4., 5.]))\n",
        "\n",
        "        # when we arrived to 2nd frame was the only time when episode was not done\n",
        "        dm = ~batch[\"done_mask\"].astype(bool)\n",
        "        assert np.all(batch[\"next_state\"][dm] == np.array(frames[[0, 0, 1]]))\n",
        "\n",
        "        # frames[2] in the history is overwritten by frames[3] because new episode has started,\n",
        "        # however it doesn't matter because the terminal state shouldn't be used anywhere.\n",
        "        assert np.all(batch[\"next_state\"][~dm] == np.array(frames[[0, 1, 3]]))\n",
        "        assert np.all((batch[\"prev_state\"] == frames[0]) | (batch[\"prev_state\"] == frames[1]))\n",
        "\n",
        "    def test_many_frames(self):\n",
        "        n_frames = 1000\n",
        "        size = 30\n",
        "        frames = np.ones((n_frames, 2, 2)).astype(\"float32\") * np.arange(n_frames).reshape(-1, 1, 1)\n",
        "        start_frame = np.ones((2, 2), \"float32\") * 10000\n",
        "        h = ExperienceHistory(\n",
        "            num_frame_stack=num_frame_stack,\n",
        "            capacity=30,\n",
        "            pic_size=pic_size\n",
        "        )\n",
        "\n",
        "        h.start_new_episode(start_frame)\n",
        "\n",
        "        #add 10 frames\n",
        "        for f in frames[:10]:\n",
        "            h.add_experience(f, 12, False, 5.0)\n",
        "\n",
        "        this_state = h.current_state()\n",
        "        h.add_experience(frames[10], 10, False, 4)\n",
        "\n",
        "        def a():\n",
        "            assert np.all(this_state == frames[7:10])\n",
        "            assert h.rewards[10] == 4\n",
        "            assert h.actions[10] == 10\n",
        "            assert not h.is_done[10]\n",
        "            assert np.all(h.frames[h.prev_states[10]] == frames[7:10])\n",
        "            assert np.all(h.frames[h.next_states[10]] == frames[8:11])\n",
        "\n",
        "        # Check that adding one frame\n",
        "        # doesn't mess up the existing history\n",
        "        a()\n",
        "\n",
        "        # add 29 more experiences and check that\n",
        "        # the past experience is not changed\n",
        "        for f in frames[11:40]:\n",
        "            done = np.random.rand() > 0.5\n",
        "            h.add_experience(f, 0, done, 1.0)\n",
        "            if done:\n",
        "                h.start_new_episode(start_frame)\n",
        "            a()\n",
        "\n",
        "        # adding one more experience should\n",
        "        # overwrite the oldest experience:\n",
        "        h.add_experience(frames[40], 1, False, 2.0)\n",
        "        assert h.rewards[10] == 2.0\n",
        "        assert h.actions[10] == 1\n",
        "        with self.assertRaises(AssertionError):\n",
        "            a()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghNYUT5w_SYH",
        "colab_type": "text"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSUc5NF298PL",
        "colab_type": "code",
        "outputId": "1dd7f8ee-c884-48a2-a3a2-a22cdc2d3c0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        }
      },
      "source": [
        "# get_ipython().magic(u'load_ext autoreload')\n",
        "# get_ipython().magic(u'autoreload 2')\n",
        "\n",
        "\"\"\"\n",
        "Python 3.5, tensorflow 1.0.0\n",
        "Trains first for train_episodes amount of episodes\n",
        "and then starts playing with the best known policy with no exploration\n",
        "Optionally save checkpoints to checkpoint_path (set checkpoint_path=None to not save anything)\n",
        "Experience history is never saved\n",
        "Training and playing can be early stopped by giving input (pressing enter in console)\n",
        "\"\"\"\n",
        "\n",
        "#from dqn.agent import CarRacingDQN\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import _thread\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# SETTINGS\n",
        "\n",
        "# to start training from scratch:\n",
        "#load_checkpoint = False\n",
        "#checkpoint_path = \"data/checkpoint02\"\n",
        "#train_episodes = float(\"inf\")\n",
        "save_freq_episodes = 400\n",
        "\n",
        "# To play from existing checkpoint without any training:\n",
        "load_checkpoint = True\n",
        "checkpoint_path = \"data/checkpoint01\"\n",
        "train_episodes = 0 #or just give higher value to train the existing checkpoint more\n",
        "\n",
        "model_config = dict(\n",
        "    min_epsilon=0.1,\n",
        "    max_negative_rewards=12,\n",
        "    min_experience_size=int(1e4),\n",
        "    num_frame_stack=3,\n",
        "    frame_skip=3,\n",
        "    train_freq=4,\n",
        "    batchsize=64,\n",
        "    epsilon_decay_steps=int(1e5),\n",
        "    network_update_freq=int(1e3),\n",
        "    experience_capacity=int(4e4),\n",
        "    gamma=0.95\n",
        ")\n",
        "\n",
        "print(model_config)\n",
        "########\n",
        "\n",
        "env_name = \"CarRacing-v0\"\n",
        "env = wrap_env(gym.make(env_name))\n",
        "\n",
        "# tf.reset_default_graph()\n",
        "dqn_agent = CarRacingDQN(env=env, **model_config)\n",
        "dqn_agent.build_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "dqn_agent.session = sess\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep=100)\n",
        "\n",
        "if load_checkpoint:\n",
        "    print(\"loading the latest checkpoint from %s\" % checkpoint_path)\n",
        "    ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
        "    assert ckpt, \"checkpoint path %s not found\" % checkpoint_path\n",
        "    global_counter = int(re.findall(\"-(\\d+)$\", ckpt.model_checkpoint_path)[0])\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    dqn_agent.global_counter = global_counter\n",
        "else:\n",
        "    if checkpoint_path is not None:\n",
        "        assert not os.path.exists(checkpoint_path), \\\n",
        "            \"checkpoint path already exists but load_checkpoint is false\"\n",
        "\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "\n",
        "def save_checkpoint():\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        os.makedirs(checkpoint_path)\n",
        "    p = os.path.join(checkpoint_path, \"m.ckpt\")\n",
        "    saver.save(sess, p, dqn_agent.global_counter)\n",
        "    print(\"saved to %s - %d\" % (p, dqn_agent.global_counter))\n",
        "\n",
        "\n",
        "def one_episode():\n",
        "    reward, frames = dqn_agent.play_episode()\n",
        "    print(\"episode: %d, reward: %f, length: %d, total steps: %d\" %\n",
        "          (dqn_agent.episode_counter, reward, frames, dqn_agent.global_counter))\n",
        "\n",
        "    save_cond = (\n",
        "        dqn_agent.episode_counter % save_freq_episodes == 0\n",
        "        and checkpoint_path is not None\n",
        "        and dqn_agent.do_training\n",
        "    )\n",
        "    if save_cond:\n",
        "        save_checkpoint()\n",
        "\n",
        "\n",
        "def input_thread(list):\n",
        "    input(\"...enter to stop after current episode\\n\")\n",
        "    list.append(\"OK\")\n",
        "\n",
        "\n",
        "def main_loop():\n",
        "    \"\"\"\n",
        "    This just calls training function\n",
        "    as long as we get input to stop\n",
        "    \"\"\"\n",
        "    list = []\n",
        "    _thread.start_new_thread(input_thread, (list,))\n",
        "    while True:\n",
        "        if list:\n",
        "            break\n",
        "        if dqn_agent.do_training and dqn_agent.episode_counter > train_episodes:\n",
        "            break\n",
        "        one_episode()\n",
        "\n",
        "    print(\"done\")\n",
        "\n",
        "\n",
        "if train_episodes > 0:\n",
        "    print(\"now training... you can early stop with enter...\")\n",
        "    print(\"##########\")\n",
        "    sys.stdout.flush()\n",
        "    main_loop()\n",
        "    save_checkpoint()\n",
        "    print(\"ok training done\")\n",
        "\n",
        "sys.stdout.flush()\n",
        "\n",
        "dqn_agent.max_neg_rewards = 100\n",
        "dqn_agent.do_training = False\n",
        "\n",
        "print(\"now just playing...\")\n",
        "print(\"##########\")\n",
        "sys.stdout.flush()\n",
        "main_loop()\n",
        "\n",
        "print(\"That's it. Good bye\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'min_epsilon': 0.1, 'max_negative_rewards': 12, 'min_experience_size': 10000, 'num_frame_stack': 3, 'frame_skip': 3, 'train_freq': 4, 'batchsize': 64, 'epsilon_decay_steps': 100000, 'network_update_freq': 1000, 'experience_capacity': 40000, 'gamma': 0.95}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "loading the latest checkpoint from data/checkpoint01\n",
            "INFO:tensorflow:Restoring parameters from data/checkpoint01/m.ckpt-152156\n",
            "now just playing...\n",
            "##########\n",
            "Track generation: 1155..1448 -> 293-tiles track\n",
            "episode: 0, reward: 906.800000, length: 311, total steps: 152156\n",
            "Track generation: 1307..1649 -> 342-tiles track\n",
            "episode: 0, reward: 762.170088, length: 334, total steps: 152156\n",
            "Track generation: 1010..1273 -> 263-tiles track\n",
            "episode: 0, reward: 903.200000, length: 323, total steps: 152156\n",
            "Track generation: 1199..1503 -> 304-tiles track\n",
            "episode: 0, reward: 853.795380, length: 334, total steps: 152156\n",
            "Track generation: 1287..1613 -> 326-tiles track\n",
            "episode: 0, reward: 746.153846, length: 334, total steps: 152156\n",
            "Track generation: 1227..1538 -> 311-tiles track\n",
            "episode: 0, reward: 819.354839, length: 334, total steps: 152156\n",
            "Track generation: 1168..1464 -> 296-tiles track\n",
            "episode: 0, reward: 866.101695, length: 334, total steps: 152156\n",
            "Track generation: 1138..1434 -> 296-tiles track\n",
            "episode: 0, reward: 866.101695, length: 334, total steps: 152156\n",
            "Track generation: 1052..1325 -> 273-tiles track\n",
            "episode: 0, reward: 892.647059, length: 334, total steps: 152156\n",
            "Track generation: 1098..1377 -> 279-tiles track\n",
            "episode: 0, reward: 889.208633, length: 334, total steps: 152156\n",
            "Track generation: 1251..1574 -> 323-tiles track\n",
            "episode: 0, reward: 791.304348, length: 334, total steps: 152156\n",
            "Track generation: 1162..1456 -> 294-tiles track\n",
            "episode: 0, reward: 886.348123, length: 334, total steps: 152156\n",
            "Track generation: 1167..1470 -> 303-tiles track\n",
            "episode: 0, reward: 827.152318, length: 334, total steps: 152156\n",
            "Track generation: 1176..1474 -> 298-tiles track\n",
            "episode: 0, reward: 889.898990, length: 334, total steps: 152156\n",
            "Track generation: 1213..1520 -> 307-tiles track\n",
            "episode: 0, reward: 844.444444, length: 334, total steps: 152156\n",
            "Track generation: 986..1240 -> 254-tiles track\n",
            "retry to generate track (normal if there are not many of this messages)\n",
            "Track generation: 1127..1413 -> 286-tiles track\n",
            "episode: 0, reward: 901.700000, length: 328, total steps: 152156\n",
            "Track generation: 1167..1467 -> 300-tiles track\n",
            "retry to generate track (normal if there are not many of this messages)\n",
            "Track generation: 1183..1483 -> 300-tiles track\n",
            "...enter to stop after current episode\n",
            "\n",
            "episode: 0, reward: 712.709030, length: 334, total steps: 152156\n",
            "done\n",
            "That's it. Good bye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVw79HAI6uGY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}